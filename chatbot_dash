import dash
import dash_bootstrap_components as dbc
from dash import html, dcc, Input, Output, State
from dash_quill import DashQuill

app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    html.H2("Chatbot with dash_quill", className="text-center my-4"),

    # Store the conversation
    dcc.Store(id="chat-store", data=[]),

    # Display conversation above the editor
    html.Div(
        id="chat-history",
        style={
            "border": "1px solid #ccc",
            "height": "300px",
            "overflowY": "auto",
            "padding": "10px",
            "marginBottom": "20px"
        }
    ),

    # Quill editor for text + images
    DashQuill(
        id="quill-editor",
        value="",  # initial content
        style={"minHeight": "150px"}
        # By default, Quill will let you paste images as base64
        # If you want to ensure the toolbar has an 'Image' button or advanced modules, see below
    ),

    dbc.Button("Send", id="send-btn", color="primary", className="mt-2"),
], fluid=True)


@app.callback(
    Output("chat-store", "data"),
    Input("send-btn", "n_clicks"),
    State("quill-editor", "value"),
    State("chat-store", "data"),
    prevent_initial_call=True
)
def on_send_click(n_clicks, quill_value, chat_data):
    """
    When user clicks Send, quill_value contains the user's combined text+images in Quill's HTML or Delta format.
    We'll treat it as a single user message, then append a bot response.
    """
    if not quill_value:
        return chat_data  # no update if empty

    # 1) Append user's message
    chat_data.append({"role": "user", "content": quill_value})

    # 2) Generate an LLM/bot response (placeholder)
    bot_reply = fake_llm_response(quill_value)
    chat_data.append({"role": "bot", "content": bot_reply})

    return chat_data


@app.callback(
    Output("chat-history", "children"),
    Input("chat-store", "data")
)
def render_chat(chat_data):
    """
    Renders the conversation. Each message is stored in 'content' which can be Quill HTML or Delta.
    dash_quill outputs HTML by default, so we can display it using dangerously_allow_html.
    """
    messages = []
    for msg in chat_data:
        if msg["role"] == "user":
            icon = html.I(className="bi bi-person-circle me-2", style={"fontSize": "1.2em"})
            style = {"textAlign": "right", "color": "#0d6efd", "marginBottom": "10px"}
        else:
            icon = html.I(className="bi bi-robot me-2", style={"fontSize": "1.2em"})
            style = {"textAlign": "left", "color": "green", "marginBottom": "10px"}

        # Display the Quill HTML
        messages.append(
            html.Div([
                icon,
                html.Div(
                    dangerously_allow_html=True,
                    children=msg["content"]
                )
            ], style=style)
        )
    return messages


def fake_llm_response(user_html):
    """
    Placeholder function. In a real app, replace with calls to
    OpenAI / Hugging Face or your own model logic.
    """
    return "<p><em>Bot Reply:</em> I see you wrote some text and/or pasted an image!</p>"


if __name__ == "__main__":
    app.run_server(debug=True)
